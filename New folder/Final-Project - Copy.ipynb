{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a2d8214",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:3.75em;color:purple; font-style:bold\"><br>\n",
    "Jupyter Notebook for the Final Project</p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a464e57d",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:2em;color:#2467C0\">Case Study: Netflix Movies and TV Shows Data Analysis</h1>\n",
    "\n",
    "### Proposed Question1: What are the most frequent genres for long-running TV shows (at least five seasons long)?\n",
    "### Proposed Question2: Can we build a machine-learning model that estimates the content rating of a Netflix movie or TV show based on all other attributes?\n",
    "### Proposed Question3: What are the most frequent themes or topics based on Netflix TV shows and movie descriptions?\n",
    "##### Disclaimer: Throughout the project, I will leverage part of the codes Dr. Porter and Dr. Altintas illustrated in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59907aa",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:2em;color:#2467C0\">Data Engineering: Step 1, Acquire Data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f45642",
   "metadata": {},
   "source": [
    "This Notebook uses a dataset from the Kaggle website. Here is the link to the data source: https://www.kaggle.com/datasets/shivamb/netflix-shows<br>\n",
    "<br>Once the download completes, please ensure the data file **netflix_titles** in the same directory this **Notebook** lives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc452f28",
   "metadata": {},
   "source": [
    "# Proposed Question1: What are the most frequent genres for long-running TV shows (at least five seasons long)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ec232a",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:2em;color:#2467C0\">Data Engineering: Step 2A, Exploring Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc3f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for long-running TV shows (more than 8 seasons)\n",
    "long_running_shows = data[(data['type'] == 'TV Show') & (data['show_seasons'] > 8)]\n",
    "\n",
    "long_running_shows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff42a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./netflix_titles.csv')\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef799356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for TV Shows\n",
    "df_tv_shows = df[df['type'] == 'TV Show']\n",
    "\n",
    "# Parse the number of seasons from the \"duration\" field\n",
    "df_tv_shows['num_seasons'] = df_tv_shows['duration'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# Filter for shows with at least 5 seasons\n",
    "df_long_running = df_tv_shows[df_tv_shows['num_seasons'] >= 5]\n",
    "\n",
    "# Split the 'listed_in' column into separate rows\n",
    "df_long_running = df_long_running.assign(listed_in=df_long_running['listed_in'].str.split(',')).explode('listed_in')\n",
    "\n",
    "# Trim whitespace\n",
    "df_long_running['listed_in'] = df_long_running['listed_in'].str.strip()\n",
    "\n",
    "df_long_running.head()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the occurrences of each genre\n",
    "genre_counts = df_long_running['listed_in'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f39464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "genre_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Genres of Long-Running TV Shows (5 Seasons or More)')\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05f645e",
   "metadata": {},
   "source": [
    "# Proposed Question2: Can we build a machine-learning model that estimates the content rating of a Netflix movie or TV show based on all other attributes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf914f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544046d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Netflix titles data from the CSV file\n",
    "data = pd.read_csv('./netflix_titles.csv')\n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f139ec",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:2em;color:#2467C0\">Data Engineering: Step 2B, Pre-Processing Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1599a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep a copy of the original data for later use, specifically for restoring the 'cast' column\n",
    "data_copy = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa1b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings for 'cast' to 'director' and 'director' to 'country'\n",
    "# These mappings are created from existing non-null data\n",
    "cast_director_mapping = data[data.director.notna()].set_index('cast')['director'].to_dict()\n",
    "director_country_mapping = data[data.country.notna()].set_index('director')['country'].to_dict()\n",
    "\n",
    "cast_director_mapping, director_country_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfa6e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing 'director' data based on the 'cast' to 'director' mapping created above\n",
    "# If the mapping does not exist for a particular cast, fill the value with 'Unknown'\n",
    "data.director = data.director.fillna(data.cast.map(cast_director_mapping))\n",
    "data.director.fillna('Unknown', inplace=True)\n",
    "\n",
    "data.director.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing 'country' data based on the 'director' to 'country' mapping created above\n",
    "# If the mapping does not exist for a particular director, fill the value with 'Unknown'\n",
    "data.country = data.country.fillna(data.director.map(director_country_mapping))\n",
    "data.country.fillna('Unknown', inplace=True)\n",
    "\n",
    "data.country.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8a7af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing 'date_added' data using the backfill method\n",
    "# The backfill method replaces missing values with the next valid value in the column\n",
    "data.date_added.bfill(inplace=True)\n",
    "\n",
    "data.date_added.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dbbba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing 'duration' data\n",
    "# These rows are not useful for the analysis and predictive model, so they are removed\n",
    "data.dropna(subset=['duration'], inplace=True)\n",
    "\n",
    "data.duration.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce91b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the position of 'date_added' for later use\n",
    "date_added_position = data.columns.get_loc('date_added')\n",
    "\n",
    "date_added_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac79fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date_added' from string to datetime format and extract the year, storing it in a new column 'year_added'\n",
    "# The year when the title was added to Netflix could be a relevant feature for the predictive model\n",
    "data['date_added'] = pd.to_datetime(data['date_added'])\n",
    "data['year_added'] = data['date_added'].dt.year\n",
    "\n",
    "data.date_added, data.year_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a2303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original 'date_added' column\n",
    "# We no longer need this column as we have extracted the year information\n",
    "data.drop('date_added', axis=1, inplace=True)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a097d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert 'year_added' at the original position of 'date_added'\n",
    "# This is done to maintain the original structure of the dataset\n",
    "data.insert(date_added_position, 'year_added', data.pop('year_added'))\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c723534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the position of 'duration' for later use\n",
    "duration_position = data.columns.get_loc('duration')\n",
    "\n",
    "duration_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3344c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'duration' to separate features 'movie_duration' and 'show_seasons'\n",
    "# This is done because the 'duration' column contains different types of information for movies and TV shows\n",
    "# For movies, it contains the duration in minutes, while for TV shows, it contains the number of seasons\n",
    "is_movie = data['duration'].str.contains('min')\n",
    "is_show = data['duration'].str.contains('Season')\n",
    "data.loc[is_movie, 'movie_duration'] = data.loc[is_movie, 'duration'].str.replace(' min', '').astype(int)\n",
    "data.loc[is_show, 'show_seasons'] = data.loc[is_show, 'duration'].str.replace(' Season(s)?', '', regex=True).astype(int)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa293cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original 'duration' column\n",
    "# We no longer need this column as we have separated the information into 'movie_duration' and 'show_seasons'\n",
    "data.drop('duration', axis=1, inplace=True)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ff4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert 'movie_duration' and 'show_seasons' at the original position of 'duration'\n",
    "# This is done to maintain the original structure of the dataset\n",
    "data.insert(duration_position, 'movie_duration', data.pop('movie_duration'))\n",
    "data.insert(duration_position + 1, 'show_seasons', data.pop('show_seasons'))\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed624be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data into two subsets:\n",
    "known_rating_data = data[data.rating.notna()].copy()\n",
    "unknown_rating_data = data[data.rating.isna()].copy()\n",
    "\n",
    "known_rating_data.isnull().sum(), unknown_rating_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d130e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the indices of the rows with unknown 'rating' for later use\n",
    "missing_rating_index = unknown_rating_data.index\n",
    "\n",
    "missing_rating_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca0552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the rows with missing 'rating' before filling\n",
    "before_filling = data_copy.loc[missing_rating_index]\n",
    "print(\"Rows with missing 'rating' before filling:\\n\", before_filling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd17e74",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:2em;color:#2467C0\">Data Analysis: Step 3, Analyze Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cacf578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'cast' column as it is not used in the predictive model\n",
    "# The 'cast' column contains too many unique values, which could make the predictive model overly complex\n",
    "data.drop('cast', axis=1, inplace=True)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40dbdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data with known 'rating' into a training set and a test set\n",
    "# The model will be trained on the training set and tested on the test set\n",
    "train_data, test_data = train_test_split(known_rating_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data.head(), test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f380f335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features to be used in the predictive model\n",
    "# These features are selected based on their relevance to the 'rating'\n",
    "features = ['type', 'director', 'country', 'year_added', 'release_year', 'movie_duration', 'show_seasons', 'listed_in']\n",
    "\n",
    "# Append a row with 'Unknown' for each feature to the training data\n",
    "# This is done to ensure that the 'Unknown' category is included in the LabelEncoder classes for each feature\n",
    "unknown_row = pd.DataFrame({feature: ['Unknown'] for feature in features}, index=[-1])\n",
    "train_data = pd.concat([train_data, unknown_row], axis=0)\n",
    "\n",
    "train_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eb3b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all features to string type, as they will be encoded as categories\n",
    "# This is necessary for the LabelEncoder to work correctly\n",
    "for feature in features:\n",
    "    train_data[feature] = train_data[feature].astype(str)\n",
    "    test_data[feature] = test_data[feature].astype(str)\n",
    "    unknown_rating_data[feature] = unknown_rating_data[feature].astype(str)\n",
    "    \n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca4380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorical variables using LabelEncoder\n",
    "# This is done to convert categorical features to numerical values, as required by the RandomForestClassifier\n",
    "le = LabelEncoder()\n",
    "for feature in features:\n",
    "    train_data[feature] = le.fit_transform(train_data[feature])\n",
    "    test_data.loc[~test_data[feature].isin(le.classes_), feature] = 'Unknown'\n",
    "    test_data[feature] = test_data[feature].map(lambda s: 'Unknown' if s not in le.classes_ else s)\n",
    "    test_data[feature] = le.transform(test_data[feature])\n",
    "    \n",
    "train_data.head(), test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c35c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the appended row from the training data\n",
    "# The 'Unknown' row was only needed for the LabelEncoder and is not used in the actual training of the model\n",
    "train_data = train_data[train_data.index != -1]\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5af60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForestClassifier on the training set\n",
    "# The RandomForestClassifier is a powerful machine learning model that can handle complex tasks\n",
    "X_train = train_data[features]\n",
    "y_train = train_data['rating']\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5293c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "# This provides an estimate of how well the model will perform on new, unseen data\n",
    "X_test = test_data[features]\n",
    "y_test = test_data['rating']\n",
    "y_pred_test = model.predict(X_test)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "print(\"Model Accuracy: \", accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060af40e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict the missing ratings in the unknown_rating_data\n",
    "# This is done by applying the trained model to the data with unknown 'rating'\n",
    "for feature in features:\n",
    "    unknown_rating_data.loc[~unknown_rating_data[feature].isin(le.classes_), feature] = 'Unknown'\n",
    "    unknown_rating_data[feature] = unknown_rating_data[feature].map(lambda s: 'Unknown' if s not in le.classes_ else s)\n",
    "    unknown_rating_data[feature] = le.transform(unknown_rating_data[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f6e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unknown = unknown_rating_data[features]\n",
    "y_pred_unknown = model.predict(X_unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ae7ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing ratings in the original data\n",
    "# The predicted ratings are used to fill the missing values in the 'rating' column\n",
    "data.loc[data.rating.isna(), 'rating'] = y_pred_unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03247c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the original 'cast' column back to the dataset\n",
    "# The 'cast' column is restored to the dataset for completeness\n",
    "data.insert(4, 'cast', data_copy['cast'])\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ad06e3",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:2em;color:#2467C0\">Data Analysis: Step 4, Reporting Insights</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d91035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the rows with missing 'rating' before filling\n",
    "# This is done to provide a comparison before and after filling the missing values\n",
    "print(\"Rows with missing 'rating' before filling:\\n\", before_filling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3188d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the rows with missing 'rating' after filling\n",
    "# This is done to provide a comparison before and after filling the missing values\n",
    "print(\"Rows with missing 'rating' after filling:\\n\", data.loc[missing_rating_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd30eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the final state of the data\n",
    "# This provides a complete view of the dataset after all preprocessing and filling of missing values\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb50206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ad7e10a",
   "metadata": {},
   "source": [
    "# Proposed Question3: What are the most frequent themes or topics based on Netflix TV shows and movie descriptions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2a33f5",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:2em;color:#2467C0\">Data Engineering: Step 2B, Pre-Processing Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "netflix_data = pd.read_csv('./netflix_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212876a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2cfb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a WordNetLemmatizer for lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Convert the tokens to lower case\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Remove punctuation from the tokens\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Remove stop words from the tokens\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Return the processed tokens\n",
    "    return tokens\n",
    "\n",
    "# Apply the text preprocessing function to the 'description' column\n",
    "netflix_data['tokens'] = netflix_data['description'].apply(preprocess_text)\n",
    "\n",
    "netflix_data['tokens'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c93f8e",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:2em;color:#2467C0\">Data Analysis: Step 3, Analyze Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d723a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# Create a dictionary from the processed tokens\n",
    "dictionary = corpora.Dictionary(netflix_data['tokens'])\n",
    "\n",
    "# Filter out tokens that occur in less than 20 documents or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "# Create a bag-of-words representation for each document\n",
    "corpus = [dictionary.doc2bow(doc) for doc in netflix_data['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf84ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set the parameters for the LDA model\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = 1  \n",
    "\n",
    "# Create an id-to-word dictionary\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "# Initialize the LDA model\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d760f3c",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:2em;color:#2467C0\">Data Analysis: Step 4, Reporting Insights</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2fa616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top topics from the LDA model\n",
    "top_topics = model.top_topics(corpus) \n",
    "\n",
    "# Print the top words for each topic\n",
    "for i, topic in enumerate(top_topics):\n",
    "    print(f'Top {i} words for topic #{i}:')\n",
    "    print([id[1] for id in topic[0][:10]])\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
